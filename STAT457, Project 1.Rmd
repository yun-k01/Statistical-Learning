---
title: "New York City Taxi Project - STAT 457 Project 1"
subtitle: "Yun Kyaw, 20177325"
output: pdf_document
date: "2023-03-27"
---

```{r setup, include=FALSE}
library(readr)
library(dplyr)
library(randomForest)
library(glmnet)
library(plotmo)
library(caret)
library(xgboost)
test_data <- read_csv("test_data.csv")  # the dataset with preprocessing from python
train_data <- read_csv("train_data.csv")  # the dataset with preprocessing from python
```

```{r}
train_data$pickup_date = as.numeric(train_data$pickup_date)  # transforming the pickup date and time to numeric
train_data$pickup_time = as.numeric(train_data$pickup_time)
test_data$pickup_date = as.numeric(test_data$pickup_date)
test_data$pickup_time = as.numeric(test_data$pickup_time)
```

```{r}
set.seed(10)
random_index <- sample(nrow(train_data), 28000) # the training set is 0.8 of the partition
df_train = train_data[random_index, ]
df_valid = train_data[-random_index, ]  # creating a validation partition to determine the performance of certain models
df_train = df_train[-c(1, 8)]  # removing the uid and fare_amount_log
df_valid = df_valid[-c(1, 8)]  # removing the uid and removing fare_amount_log
df_test = test_data[-1]  # removing the uid
```

### Exploratory Data Analysis - Look at the fare given the number of riders
```{r}
## Question 4
W23P1_train <- read_csv("W23P1_train.csv")
```

```{r}
passengers_fare = data.frame(fare = W23P1_train$fare_amount,
                            passengers =  W23P1_train$passenger_count)
passengers_fare$passengers = ordered(passengers_fare$passengers, 
                                     levels = c('1', '2', '3', '4', '5', '6'))
```

```{r}
# looking at the anova
fare_aov <- aov(fare ~ passengers, data = passengers_fare)
summary(fare_aov)

# looking at the distribution of the different passengers and their fare
EDA_Q4 = group_by(passengers_fare, passengers) %>%
  summarise(
    count = n(),
    mean = mean(fare),
    sd = sd(fare)
  )
```


### Creating prediction model 1 - Multiple Linear Regression
```{r}
# doesnt require validation, can use train_data dataset
myfit1 = lm(fare_amount ~ ., data = train_data[-c(1, 8)])
summary(myfit1)
```
#### Testing the Accuracy (RMSE)
```{r}
rmse1 = sqrt(mean(myfit1$residuals^2))
# rmse1 = 4.441584
```

#### Updating the Model - Stepwise Regression
```{r}
full = myfit1  # fit of the full model
null = lm(fare_amount ~ 1, data = train_data[-c(1, 8)])  # fit of just the residuals
sfit = step(null, scope = list(lower = null, upper = full), direction = 'both')
summary(sfit) # fit = fare_amount ~ dist_longitude + dist_latitude + pickup_date
sfit_rmse = sqrt(mean(sfit$residuals^2))
# sfit_rmse = 4.441662
```


#### Updating the Model - Lasso and Ridge Regression (Not Included in Report)
```{r}
# Lasso - utilize the training partition to perform validation of results
x_var = as.matrix(df_train[-1])
y_var = as.vector(df_train$fare_amount)
lassofit = glmnet(x = x_var, y = df_train$fare_amount, alpha = 1)
plot_glmnet(lassofit, label = TRUE, xvar= "lambda")

# selecting lambda using cross validation
lasso.cv.out = cv.glmnet(x = x_var, y = df_train$fare_amount, alpha = 1) 
plot(lasso.cv.out)

# one-standard deviation rule
Ytest.1se = predict(lasso.cv.out, s = lasso.cv.out$lambda.1se, newx = as.matrix(df_valid[-1]))
lasso1se = sqrt(mean((Ytest.1se - df_valid$fare_amount)^2))
# lasso1se = 4.880131

# smallest cross validation error
Ytest.min = predict(lasso.cv.out, s = lasso.cv.out$lambda.min, newx=as.matrix(df_valid[-1]))
lassomin = sqrt(mean((Ytest.min - df_valid$fare_amount)^2))
# lassomin = 4.708717
```

```{r}
# Ridge
ridgefit = glmnet(x = x_var, y = df_train$fare_amount, alpha = 0)
plot_glmnet(myfit4, label = TRUE, xvar = "lambda")

# use cv to select lambda
ridge.cv.out = cv.glmnet(x_var, df_train$fare_amount, alpha = 0) 
plot(ridge.cv.out)

lam.seq = exp(seq(-5, 3, length=100))
ridge.cv.out = cv.glmnet(x = x_var, y = df_train$fare_amount, alpha = 0, lambda = lam.seq) 
plot(ridge.cv.out)

Ytest.1se = predict(ridge.cv.out, s = ridge.cv.out$lambda.1se, newx=as.matrix(df_valid[-1]))
ridge1se = sqrt(mean((Ytest.1se - df_valid$fare_amount)^2))
# ridge1se = 4.892166	
Ytest.min = predict(ridge.cv.out, s = ridge.cv.out$lambda.min, newx=as.matrix(df_valid[-1]))
ridgemin = sqrt(mean((Ytest.min - df_valid$fare_amount)^2))
# ridgemin = 4.708502
```

Looking at the RMSE of the above 4 Models:
```{r}
regr1_rmse = data.frame(Regression_Model = c('Linear', 'Stepwise', 'Lasso_1se', 'Lasso_min', 'Ridge_1se', 'Ridge_min'), RMSE = c(rmse1, sfit_rmse, lasso1se, lassomin, ridge1se, ridgemin))
regr1_rmse
```
Here we find that the linear and stepwise regressions performed the best, hence why they were included in the report.


### Creating Prediction Model 2 - Random Forest Regression
```{r}
rfModel = randomForest(fare_amount ~ ., data = train_data[-c(1, 8)], importance = TRUE, ntree= 1000)
```
#### Testing the Accuracy (RMSE)
```{r}
rf.pred = predict(rfModel, df_valid[-1])
rf_rmse = sqrt(mean((rf.pred - df_valid$fare_amount)^2))
```



### Creating prediction model 3 - Boosting Model for Regression
```{r}
# training a boosting model
ctrl = trainControl(method = "repeatedcv",
                    number = 10,
                    repeats = 5)
gbm.Grid = expand.grid(interaction.depth = c(2,3,4,5), 
                       n.trees = (1:5)*200, 
                       shrinkage = c(0.1, 0.05),
                       n.minobsinnode = 10) 
gbm.cv.model <- train(fare_amount ~ ., data = df_train,
                      method = "gbm",
                      trControl = ctrl,
                      tuneGrid = gbm.Grid,
                      verbose = FALSE)
```
```{r}
gbm.cv.model
```
#### Testing the Accuracy (RMSE) 
```{r}
# looking at the overall predicted rmse of the training model
boost.pred = predict(gbm.cv.model, newdata = df_valid[-1])
boost.predrmse = sqrt(mean((boost.pred - df_valid$fare_amount)^2))
# boost.predrmse = 4.310648

# looking at the smallest RMSE from model, and its conditions
boost_rmse = min(gbm.cv.model$results[5])
# this occurs at index 16, where shrinkage = 0.05, interaction.depth = 5, and n.trees = 200 with an RMSE of 3.882787
```
#### Updating the Model - GBM
```{r}
boost_data = as.data.frame(df_train)
boost_valid = as.data.frame(df_valid)
boost_test = as.data.frame(df_test)
```

```{r}
set.seed(10)
boostModel = gbm(fare_amount ~ ., data = boost_data,
                 distribution = "gaussian",
                 n.trees = 99,
                 interaction.depth = 3, 
                 cv.folds = 5)
cv.num = gbm.perf(boostModel)
yhat.boost.cv = predict(boostModel, newdata = boost_valid[-1], n.trees = cv.num)
boostTree_rmse = sqrt(mean((yhat.boost.cv - unlist(boost_valid[1]))^2))
# boostTree_rmse = 4.369499
```
##### Other GBM models that were tested
```{r}
set.seed(10)
gbm_model1 = gbm(fare_amount ~ ., data = boost_data,
                 distribution = "gaussian",
                 n.trees = 200,  # set n.trees = 200
                 interaction.depth = 5,   # set interaction depth = 5
                 cv.folds = 5)

cv.num1 = gbm.perf(gbm_model1)
# find n.trees = 89 under cross validation
yhat.boost.cv1 = predict(gbm_model1, newdata = boost_valid[-1], n.trees = cv.num1)
gbm_rmse1 = sqrt(mean((yhat.boost.cv1 - unlist(boost_valid[1]))^2))
# gbm_rmse1 = 4.301632

set.seed(10)
gbm_model2 = gbm(fare_amount ~ ., data = boost_data,
                 distribution = "gaussian",
                 n.trees = 89,
                 interaction.depth = 5, 
                 cv.folds = 5)

cv.num2 = gbm.perf(gbm_model2)  # confirms optimal number of trees at 89
yhat.boost.cv2 = predict(gbm_model2, newdata = boost_valid[-1], n.trees = cv.num2)
gbm_rmse2 = sqrt(mean((yhat.boost.cv2 - unlist(boost_valid[1]))^2))
# gbm_rmse2 = 4.301632
```

#### Updating the Model - xgBoost
```{r}
xgb_train = xgb.DMatrix(data = as.matrix(boost_data[-1]), label = unlist(boost_data$fare_amount))
xgb_valid = xgb.DMatrix(data = as.matrix(boost_valid[-1]), label = unlist(boost_valid$fare_amount))
xgb_test = xgb.DMatrix(data = as.matrix(boost_test))
# in this model we will use the optimal conditions from the first boosting regression (shrinkage = 0.05, interaction.depth = 5, and n.trees = 200)
xgboostModel = xgboost(data = xgb_train, shrinkage = 0.05, interaction.depth = 5, nrounds = 200)
xgboost_rmse = min(xgboostModel$evaluation_log[,2])
# xgboost_rmse = 2.015966 
```

### Looking at the Overall Accuracies and Creating the Predictions
```{r}
overall_rmse = data.frame(Regression_Model = c('Linear', 'Stepwise', 'RandomForest', 'gbm', 'xgBoost'), RMSE = c(rmse1, sfit_rmse, rf_rmse, boost_rmse, xgboost_rmse))
overall_rmse
```
Thus, we observe that the xgBoost is the best model, and the worst model was the stepwise model.

#### Creating Predictions

```{r}
# linear regression -- submitted
linear_test = predict(myfit1, new_data = test_data[-1])
linear_predictions = data.frame(uid = test_data$uid, fare_amount = linear_test)
write.csv(linear_predictions, "linear_predictions.csv", row.names = F)

# stepwise regression
stepwise_test = predict(sfit, new_data = test_data[-1])
stepwise_predictions = data.frame(uid = test_data$uid, fare_amount = stepwise_test)
write.csv(stepwise_predictions, "stepwise_predictions.csv", row.names = F)

# random forest  -- submitted
rf_test = predict(rfModel, new_data = test_data[-1])
rf_predictions = data.frame(uid = test_data$uid, fare_amount = rf_test)
write.csv(rf_predictions, "rf_predictions.csv", row.names = F)

# boosting model -- submitted
boost_tree = predict(boostModel, new_data = test_data[-1])
boost_predictions1 = data.frame(uid = test_data$uid, fare_amount = boost_tree)
write.csv(boost_predictions1, "boost_tree_predictions.csv", row.names = F)

# xgboost -- submitted
xgboost_test = predict(xgboostModel, xgb_test)
xgboost_predictions = data.frame(uid = test_data$uid, fare_amount = xgboost_test)
write.csv(xgboost_predictions, "xgboost_predictions.csv", row.names = F)
```
