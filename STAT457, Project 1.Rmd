---
title: "New York City Taxi Project - STAT 457 Project 1"
subtitle: "Yun Kyaw, 20177325"
output: pdf_document
date: "2023-03-27"
---

```{r setup, include=FALSE}
library(readr)
library(dplyr)
library(randomForest)
library(glmnet)
library(plotmo)
library(caret)
test_data <- read_csv("Desktop/STAT/STAT 457/STAT 457 Proj 1/test_data.csv")
train_data <- read_csv("Desktop/STAT/STAT 457/STAT 457 Proj 1/train_data.csv")
```

```{r}
train_data$pickup_date = as.numeric(train_data$pickup_date)
train_data$pickup_time = as.numeric(train_data$pickup_time)
test_data$pickup_date = as.numeric(test_data$pickup_date)
test_data$pickup_time = as.numeric(test_data$pickup_time)
```

```{r}
set.seed(10)
random_index <- sample(nrow(train_data), 28000) # the training set is .8 of the partition
df_train = train_data[random_index, ]
df_valid = train_data[-random_index, ]
df_train = df_train[-8]  # removing fare_amount_log
df_valid = df_valid[-8]  # removing fare_amount_log
train_data = train_data[-8]
```

### Creating prediction model 1 - Multiple Linear Regression
```{r}
# doesnt require validation, can use train_data dataset
myfit1 = lm(fare_amount ~ ., data = train_data)
summary(myfit1)
rmse1 = sqrt(mean(myfit1$residuals^2))
```
#### Testing the Accuracy (RMSE)
```{r}
rmse1 = sqrt(mean(myfit1$residuals^2))
```

#### Updating the Model - Stepwise Regression
```{r}
full = myfit1
null = lm(fare_amount ~ 1, data = train_data)
sfit = step(null, scope = list(lower = null, upper = full), direction = 'both')
summary(sfit)
sfit_rmse = sqrt(mean(sfit$residuals^2))
```

#### Updating the Model - Lasso and Ridge Regression (Not Included in Report)
```{r}
# Lasso
x_var = as.matrix(df_train[-2])
y_var = as.vector(df_train$fare_amount)
lassofit = glmnet(x = x_var, y = df_train$fare_amount, alpha = 1)
plot_glmnet(lassofit, label = TRUE, xvar= "lambda")

# selecting lambda using cross validation
lasso.cv.out = cv.glmnet(x = x_var, y = df_train$fare_amount, alpha = 1) 
plot(lasso.cv.out)

# one-standard deviation rule
Ytest.1se = predict(lasso.cv.out, s = lasso.cv.out$lambda.1se, newx = as.matrix(df_valid[-2]))
lasso1se = sqrt(mean((Ytest.1se - df_valid$fare_amount)^2))

# smallest cross validation error
Ytest.min = predict(lasso.cv.out, s = lasso.cv.out$lambda.min, newx=as.matrix(df_valid[-2]))
lassomin = sqrt(mean((Ytest.min - df_valid$fare_amount)^2))
```

```{r}
# Ridge
ridgefit = glmnet(x = x_var, y = df_train$fare_amount, alpha = 0)
plot_glmnet(myfit4, label = TRUE, xvar = "lambda")

# use cv to select lambda
ridge.cv.out = cv.glmnet(x_var, df_train$fare_amount, alpha = 0) 
plot(ridge.cv.out)

lam.seq = exp(seq(-5, 3, length=100))
ridge.cv.out = cv.glmnet(x = x_var, y = df_train$fare_amount, alpha = 0, lambda = lam.seq) 
plot(ridge.cv.out)

ridge.cv.out$lambda.min
ridge.cv.out$lambda.1se

Ytest.1se = predict(ridge.cv.out, s = ridge.cv.out$lambda.1se, newx=as.matrix(df_valid[-2]))
ridge1se = sqrt(mean((Ytest.1se - df_valid$fare_amount)^2))

Ytest.min = predict(ridge.cv.out, s = ridge.cv.out$lambda.min, newx=as.matrix(df_valid[,-2]))
ridgemin = sqrt(mean((Ytest.min - df_valid$fare_amount)^2))
```

Looking at the RMSE of the above 4 Models:
```{r}
regr1_rmse = data.frame(Regression_Model = c('Linear', 'Stepwise', 'Lasso_1se', 'Lasso_min', 'Ridge_1se', 'Ridge_min'), RMSE = c(rmse1, sfit_rmse, lasso1se, lassomin, ridge1se, ridgemin))
regr1_rmse
```
Here we find that the linear and stepwise regressions performed the best


### Creating Prediction Model 2 - Random Forest Regression
```{r}
rfModel = randomForest(fare_amount ~ ., data = df_train, importance = TRUE, ntree= 1000)
```
#### Testing the Accuracy (RMSE)
```{r}
rf_rmse = sqrt(min(rfModel$mse))
```


### Creating prediction model 3 - Boosting Model for Regression
```{r}
ctrl = trainControl(method = "repeatedcv",
                    number = 10,
                    repeats = 5)
gbm.Grid = expand.grid(interaction.depth = c(2,3,4,5), 
                       n.trees = (1:5)*200, 
                       shrinkage = c(0.1, 0.05),
                       n.minobsinnode = 10) 

gbm.cv.model <- train(fare_amount ~ ., data = df_train,
                      method = "gbm",
                      trControl = ctrl,
                      tuneGrid = gbm.Grid,
                      verbose = FALSE)
boost.pred = predict(gbm.cv.model, newdata = df_valid[,-2] )
boost.predrmse = sqrt(mean((boost.pred - df_valid$fare_amount)^2))
```
#### Testing the Accuracy (RMSE) 
```{r}
boost_rmse = min(gbm.cv.model$results[5])
# find the minimized RMSE exists when shrinkage = 0.05, interaction.depth = 5, n.trees = 200
```

#### Updating the Model - xgBoost
```{r}
library(xgboost)
boost_data = as.data.frame(df_train)
boost_valid = as.data.frame(df_valid)
boost_test = as.data.frame(test_data)
xgb_train = xgb.DMatrix(data = as.matrix(boost_data[-2]), label = unlist(boost_data[2]))
xgb_valid = xgb.DMatrix(data = as.matrix(boost_valid[-2]), label = unlist(boost_valid[2]))
xgb_test = xgb.DMatrix(data = as.matrix(boost_test))

xgboostModel = xgboost(data = xgb_train, shrinkage = 0.05, interaction.depth = 5, nrounds = 200)
xgboost_rmse = min(xgboostModel$evaluation_log[,2])
```

### Looking at the Overall Accuracies and Creating the Predictions
```{r}
overall_rmse = data.frame(Regression_Model = c('Linear', 'Lasso', 'Ridge', 'RandomForest', 'Boosting', 'xgBoost'), RMSE = c(rmse1, rmse2, rmse3, rf_rmse, boost_rmse, xgboost_rmse))
overall_rmse
```
Thus, we observe that the xgBoost is the best model, and the worst model was the stepwise model

#### Creating Predictions
```{r}
# linear regression
linear_test = predict(myfit1, new_data = test_data)
linear_predictions = data.frame(uid = test_data$uid, fare_amount = linear_test)
write.csv(linear_predictions, "linear_predictions.csv", row.names = F)

# stepwise regression
stepwise_test = predict(sfit, new_data = test_data)
stepwise_predictions = data.frame(uid = test_data$uid, fare_amount = stepwise_test)
write.csv(stepwise_predictions, "stepwise_predictions.csv", row.names = F)

# random forest
rf_test = predict(rfModel, new_data = test_data)
rf_predictions = data.frame(uid = test_data$uid, fare_amount = rf_test)
write.csv(rf_predictions, "rf_predictions.csv", row.names = F)

# boosting
boost_test = predict(gbm.cv.model, new_data = test_data)
boost_predictions = data.frame(uid = test_data$uid, fare_amount = boost_test)
write.csv(boost_predictions, "boost_predictions.csv", row.names = F)

# xgboost
xgboost_test = predict(xgboostModel, new_data = test_data)
xgboost_predictions = data.frame(uid = test_data$uid, fare_amount = xgboost_test)
write.csv(xgboost_predictions, "xgboost_predictions.csv", row.names = F)
```
